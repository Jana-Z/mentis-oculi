<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!-- Primary Meta Tags -->
    <!-- TODO: Replace with your paper title and author names -->
    <meta name="title"
        content="MentisOculi: Revealing the Limits of Reasoning with Mental Imagery – Jana Zeller, Thaddäus Wiedemer, Fanfei Li, Thomas Klein, Matthias Bethge, Felix Wichmann, Ryan Cotterell, Wieland Brendel">
    <!-- TODO: Write a compelling 150-160 character description of your research -->
    <meta name="description"
        content="Frontier models are transitioning from multimodal large language models (MLLMs) that merely ingest visual information to unified multimodal models (UMMs) capable of native interleaved generation. This shift has sparked interest in using intermediate visualizations as a reasoning aid, akin to human mental imagery. Central to this idea is the ability to form, maintain, and manipulate visual representations in a goal-oriented manner. To evaluate and probe this capability, we develop MentisOculi, a procedural, stratified suite of multi-step reasoning problems amenable to visual solution, tuned to challenge frontier models. Evaluating visual strategies ranging from latent tokens to explicit generated imagery, we find they generally fail to improve performance. Analysis of UMMs specifically exposes a critical limitation: While they possess the textual reasoning capacity to solve a task and can sometimes generate correct visuals, they suffer from compounding generation errors and fail to leverage even ground-truth visualizations. Our findings suggest that despite their inherent appeal, visual thoughts do not yet benefit model reasoning. MentisOculi establishes the necessary foundation to analyze and close this gap across diverse model families.">
    <!-- TODO: Add 5-10 relevant keywords for your research area -->
    <meta name="keywords" content="mental imagery,visual reasoning,reasoning,llms,mllms,umms,video models">
    <!-- TODO: List all authors -->
    <meta name="author"
        content="Jana Zeller, Thaddäus Wiedemer, Fanfei Li, Thomas Klein, Matthias Bethge, Felix Wichmann, Ryan Cotterell, Wieland Brendel">
    <meta name="robots" content="index, follow">
    <meta name="language" content="English">

    <!-- Open Graph / Facebook -->
    <meta property="og:type" content="article">
    <!-- TODO: Replace with your institution or lab name -->
    <meta property="og:site_name"
        content="MPI for Intelligent Systems, ELLIS Institute Tübingen, ETH Zurich, University of Tuebingen">
    <!-- TODO: Same as paper title above -->
    <meta property="og:title" content="MentisOculi: Revealing the Limits of Reasoning with Mental Imagery">
    <!-- TODO: Same as description above -->
    <meta property="og:description"
        content="Frontier models are transitioning from multimodal large language models (MLLMs) that merely ingest visual information to unified multimodal models (UMMs) capable of native interleaved generation. This shift has sparked interest in using intermediate visualizations as a reasoning aid, akin to human mental imagery. Central to this idea is the ability to form, maintain, and manipulate visual representations in a goal-oriented manner. To evaluate and probe this capability, we develop MentisOculi, a procedural, stratified suite of multi-step reasoning problems amenable to visual solution, tuned to challenge frontier models. Evaluating visual strategies ranging from latent tokens to explicit generated imagery, we find they generally fail to improve performance. Analysis of UMMs specifically exposes a critical limitation: While they possess the textual reasoning capacity to solve a task and can sometimes generate correct visuals, they suffer from compounding generation errors and fail to leverage even ground-truth visualizations. Our findings suggest that despite their inherent appeal, visual thoughts do not yet benefit model reasoning. MentisOculi establishes the necessary foundation to analyze and close this gap across diverse model families.">
    <!-- TODO: Replace with your actual website URL -->
    <meta property="og:url" content="https://jana-z.github.io/mentis-oculis">
    <!-- TODO: Create a 1200x630px preview image and update path -->
    <meta property="og:image" content="https://YOUR_DOMAIN.com/static/images/social_preview.png">
    <meta property="og:image:width" content="1200">
    <meta property="og:image:height" content="630">
    <meta property="og:image:alt"
        content="MentisOculi: Revealing the Limits of Reasoning with Mental Imagery - Research Preview">
    <meta property="article:published_time" content="2026-01-31T00:00:00.000Z">
    <meta property="article:author" content="Zeller, Jana">
    <meta property="article:section" content="Research">
    <meta property="article:tag" content="mental imagery">
    <meta property="article:tag" content="visual reasoning">
    <meta property="article:tag" content="reasoning">
    <meta property="article:tag" content="llms">
    <meta property="article:tag" content="mllms">
    <meta property="article:tag" content="umms">
    <meta property="article:tag" content="video models">

    <!-- Twitter -->
    <meta name="twitter:card" content="summary_large_image">
    <!-- TODO: Replace with your lab/institution Twitter handle -->
    <meta name="twitter:site" content="@wielandbr">
    <!-- TODO: Replace with first author's Twitter handle -->
    <meta name="twitter:creator" content="@Jana46052173">
    <!-- TODO: Same as paper title above -->
    <meta name="twitter:title" content="MentisOculi: Revealing the Limits of Reasoning with Mental Imagery">
    <!-- TODO: Same as description above -->
    <meta name="twitter:description"
        content="Frontier models are transitioning from multimodal large language models (MLLMs) that merely ingest visual information to unified multimodal models (UMMs) capable of native interleaved generation. This shift has sparked interest in using intermediate visualizations as a reasoning aid, akin to human mental imagery. Central to this idea is the ability to form, maintain, and manipulate visual representations in a goal-oriented manner. To evaluate and probe this capability, we develop MentisOculi, a procedural, stratified suite of multi-step reasoning problems amenable to visual solution, tuned to challenge frontier models. Evaluating visual strategies ranging from latent tokens to explicit generated imagery, we find they generally fail to improve performance. Analysis of UMMs specifically exposes a critical limitation: While they possess the textual reasoning capacity to solve a task and can sometimes generate correct visuals, they suffer from compounding generation errors and fail to leverage even ground-truth visualizations. Our findings suggest that despite their inherent appeal, visual thoughts do not yet benefit model reasoning. MentisOculi establishes the necessary foundation to analyze and close this gap across diverse model families.">
    <!-- TODO: Same as social preview image above -->
    <meta name="twitter:image" content="https://YOUR_DOMAIN.com/static/images/social_preview.png">
    <meta name="twitter:image:alt"
        content="MentisOculi: Revealing the Limits of Reasoning with Mental Imagery - Research Preview">

    <!-- Academic/Research Specific -->
    <meta name="citation_title" content="MentisOculi: Revealing the Limits of Reasoning with Mental Imagery">
    <meta name="citation_author" content="Zeller, Jana">
    <meta name="citation_author" content="Wiedemer, Thaddäus">
    <meta name="citation_author" content="Li, Fanfei">
    <meta name="citation_author" content="Klein, Thomas">
    <meta name="citation_author" content="Bethge, Matthias">
    <meta name="citation_author" content="Wichmann, Felix">
    <meta name="citation_author" content="Cotterell, Ryan">
    <meta name="citation_author" content="Brendel, Wieland">
    <!-- <meta name="citation_publication_date" content="2026">
  <meta name="citation_conference_title" content="ICML"> -->
    <!-- <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=q66P8XD3L4"> -->

    <!-- Additional SEO -->
    <meta name="theme-color" content="#2563eb">
    <meta name="msapplication-TileColor" content="#2563eb">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="default">

    <!-- Preconnect for performance -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link rel="preconnect" href="https://ajax.googleapis.com">
    <link rel="preconnect" href="https://documentcloud.adobe.com">
    <link rel="preconnect" href="https://cdn.jsdelivr.net">


    <!-- TODO: Replace with your paper title and authors -->
    <title>MentisOculi: Revealing the Limits of Reasoning with Mental Imagery - Jana Zeller, Thaddäus Wiedemer, Fanfei
        Li, Thomas Klein, Matthias Bethge, Felix Wichmann, Ryan Cotterell, Wieland Brendel | Academic Research</title>

    <!-- Favicon and App Icons -->
    <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
    <link rel="apple-touch-icon" href="static/images/favicon.ico">

    <!-- Critical CSS - Load synchronously -->
    <link rel="stylesheet" href="static/css/bulma.min.css">
    <link rel="stylesheet" href="static/css/index.css">

    <!-- Non-critical CSS - Load asynchronously -->
    <link rel="preload" href="static/css/bulma-carousel.min.css" as="style"
        onload="this.onload=null;this.rel='stylesheet'">
    <link rel="preload" href="static/css/bulma-slider.min.css" as="style"
        onload="this.onload=null;this.rel='stylesheet'">
    <link rel="preload" href="static/css/fontawesome.all.min.css" as="style"
        onload="this.onload=null;this.rel='stylesheet'">
    <link rel="preload" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css" as="style"
        onload="this.onload=null;this.rel='stylesheet'">

    <!-- Fallback for browsers that don't support preload -->
    <noscript>
        <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
        <link rel="stylesheet" href="static/css/bulma-slider.min.css">
        <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    </noscript>

    <!-- Fonts - Optimized loading -->
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700;800&display=swap" rel="stylesheet">

    <!-- Defer non-critical JavaScript -->
    <script defer src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script defer src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
    <script defer src="static/js/fontawesome.all.min.js"></script>
    <script defer src="static/js/bulma-carousel.min.js"></script>
    <script defer src="static/js/bulma-slider.min.js"></script>
    <script defer src="static/js/index.js"></script>

    <!-- Structured Data for Academic Papers -->
    <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "ScholarlyArticle",
    "headline": "MentisOculi: Revealing the Limits of Reasoning with Mental Imagery",
    "description": "Frontier models are transitioning from multimodal large language models (MLLMs) that merely ingest visual information to unified multimodal models (UMMs) capable of native interleaved generation. This shift has sparked interest in using intermediate visualizations as a reasoning aid, akin to human mental imagery. Central to this idea is the ability to form, maintain, and manipulate visual representations in a goal-oriented manner. To evaluate and probe this capability, we develop MentisOculi, a procedural, stratified suite of multi-step reasoning problems amenable to visual solution, tuned to challenge frontier models. Evaluating visual strategies ranging from latent tokens to explicit generated imagery, we find they generally fail to improve performance. Analysis of UMMs specifically exposes a critical limitation: While they possess the textual reasoning capacity to solve a task and can sometimes generate correct visuals, they suffer from compounding generation errors and fail to leverage even ground-truth visualizations. Our findings suggest that despite their inherent appeal, visual thoughts do not yet benefit model reasoning. MentisOculi establishes the necessary foundation to analyze and close this gap across diverse model families.",
    "author": [
      {
        "@type": "Person",
        "name": "Jana Zeller",
        "affiliation": {
          "@type": "Organization",
          "name": "MPI for Intelligent Systems, ELLIS Institute Tübingen, ETH Zurich"
        }
      },
      {
        "@type": "Person",
        "name": "Thaddäus Wiedemer",
        "affiliation": {
          "@type": "Organization",
          "name": "MPI for Intelligent Systems, ELLIS Institute Tübingen, University of Tuebingen"
        }
      },
      {
        "@type": "Person",
        "name": "Fanfei Li",
        "affiliation": {
          "@type": "Organization",
          "name": "MPI for Intelligent Systems, ELLIS Institute Tübingen, University of Tuebingen"
        }
      },
      {
        "@type": "Person",
        "name": "Thomas Klein",
        "affiliation": {
          "@type": "Organization",
          "name": "MPI for Intelligent Systems, ELLIS Institute Tübingen, University of Tuebingen"
        }
      },
      {
        "@type": "Person",
        "name": "Matthias Bethge",
        "affiliation": {
          "@type": "Organization",
          "name": "University of Tuebingen"
        }
      },
      {
        "@type": "Person",
        "name": "Felix Wichmann",
        "affiliation": {
          "@type": "Organization",
          "name": "University of Tuebingen"
        }
      },
      {
        "@type": "Person",
        "name": "Ryan Cotterell",
        "affiliation": {
          "@type": "Organization",
          "name": "ETH Zurich"
        }
      },
      {
        "@type": "Person",
        "name": "Wieland Brendel",
        "affiliation": {
          "@type": "Organization",
          "name": "MPI for Intelligent Systems, ELLIS Institute Tübingen"
        }
      }
    ],
    "datePublished": "2026-01-30",
    "publisher": {
      "@type": "Organization",
      "name": ""
    },
    "url": "https://jana-z.github.io/mentis-oculis.github.io/",
    "image": "https://jana-z.github.io/mentis-oculis.github.io/static/images/social_preview.png",
    "keywords": ["mental imagery", "visual reasoning", "reasoning", "llms", "mllms", "umms", "video models"],
    "abstract": "Frontier models are transitioning from multimodal large language models (MLLMs) that merely ingest visual information to unified multimodal models (UMMs) capable of native interleaved generation. This shift has sparked interest in using intermediate visualizations as a reasoning aid, akin to human mental imagery. Central to this idea is the ability to form, maintain, and manipulate visual representations in a goal-oriented manner. To evaluate and probe this capability, we develop MentisOculi, a procedural, stratified suite of multi-step reasoning problems amenable to visual solution, tuned to challenge frontier models. Evaluating visual strategies ranging from latent tokens to explicit generated imagery, we find they generally fail to improve performance. Analysis of UMMs specifically exposes a critical limitation: While they possess the textual reasoning capacity to solve a task and can sometimes generate correct visuals, they suffer from compounding generation errors and fail to leverage even ground-truth visualizations. Our findings suggest that despite their inherent appeal, visual thoughts do not yet benefit model reasoning. MentisOculi establishes the necessary foundation to analyze and close this gap across diverse model families.",
    "citation": "{
        "@article{zeller2026mentisoculi,
            title={{MENTISOCULI}: Revealing the Limits of Reasoning with Mental Imagery},
            author={Zeller, Jana and Wiedemer, Thadd{\"a}us and Li, Fanfei and Klein, Thomas and Mayilvahanan, Prasanna and Bethge, Matthias and Wichmann, Felix and Cotterell, Ryan and Brendel, Wieland},
            journal={arXiv preprint},
            year={2026},
            note={Preprint. January 31, 2026}
        }
    }",
    "isAccessibleForFree": true,
    "license": "https://creativecommons.org/licenses/by/4.0/",
    "mainEntity": {
      "@type": "WebPage",
      "@id": ""
    },
    "about": [
      {
        "@type": "Thing",
        "name": "Visual Reasoning"
      },
      {
        "@type": "Thing", 
        "name": "Mental Imagery"
      },
      {
        "@type": "Thing",
        "name": "LLMs"
      },
      {
        "@type": "Thing",
        "name": "UMMs"
      },
      {
        "@type": "Thing",
        "name": "Video Models"
      }
    ]
  }
  </script>

    <!-- Website/Organization Structured Data -->
    <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "Organization",
    "name": "MPI for Intelligent Systems",
    "url": "https://is.mpg.de",
    "logo": "https://jana-z.github.io/mentis-oculis/static/images/favicon.ico",
    "sameAs": [
      "https://x.com/MPI_IS"
    ]
  }
  </script>
</head>

<body>


    <!-- Scroll to Top Button -->
    <button class="scroll-to-top" onclick="scrollToTop()" title="Scroll to top" aria-label="Scroll to top">
        <i class="fas fa-chevron-up"></i>
    </button>

    <main id="main-content">
        <section class="hero">
            <div class="hero-body">
                <div class="container is-max-desktop">
                    <div class="columns is-centered">
                        <div class="column has-text-centered">
                            <!-- TODO: Replace with your paper title -->
                            <h1 class="title is-1 publication-title">MentisOculi: Revealing the Limits of Reasoning with
                                Mental Imagery</h1>
                            <div class="is-size-5 publication-authors">
                                <!-- TODO: Replace with your paper authors and their personal links -->
                                <span class="author-block">
                                    <a href="https://jana-z.github.io" target="_blank">Jana
                                        Zeller</a><sup>1, 2, 3</sup>,</span>
                                <span class="author-block">
                                    <a href="https://thwiedemer.com" target="_blank">Thaddäus
                                        Wiedemer</a><sup>1, 2, 4</sup>,</span>
                                <span class="author-block">
                                    <a href="https://www.linkedin.com/in/fanfei-li/" target="_blank">Fanfei
                                        Li</a><sup>1, 2, 4</sup>,
                                </span>
                                <span class="author-block">
                                    <a href="https://www.linkedin.com/in/thomas-klein-123795159/?originalSubdomain=de"
                                        target="_blank">Thomas Klein</a><sup>1, 2,
                                        4</sup>,
                                </span>
                                <span class="author-block">
                                    <a href="https://www.prasannamayil.com" target="_blank">Prasanna
                                        Mayilvahanan</a><sup>1, 2,
                                        4</sup>,
                                </span>
                                <span class="author-block">
                                    <a href="https://bethgelab.org" target="_blank">Matthias Bethge</a><sup>4</sup>,
                                </span>
                                <span class="author-block">
                                    <a href="https://uni-tuebingen.de/fakultaeten/mathematisch-naturwissenschaftliche-fakultaet/fachbereiche/informatik/lehrstuehle/neuronale-informationsverarbeitung/home/"
                                        target="_blank">Felix Wichmann</a><sup>4</sup>,
                                </span>
                                <span class="author-block">
                                    <a href="https://rycolab.io/authors/ryan/" target="_blank">Ryan
                                        Cotterell</a><sup>3</sup>,
                                </span>
                                <span class="author-block">
                                    <a href="https://brendel-group.github.io" target="_blank">Wieland
                                        Brendel</a><sup>1, 2, 4</sup>
                                </span>
                            </div>

                            <div class="is-size-5 publication-authors">
                                <!-- TODO: Replace with your institution and conference/journal info -->
                                <span class="author-block"><sup>1</sup>MPI for Intelligent Systems,
                                    <sup>2</sup>ELLIS Institute Tübingen, <sup>3</sup>ETH Zurich</span>,
                                <sup>4</sup>University of Tuebingen
                            </div>

                            <div class="column has-text-centered">
                                <div class="publication-links">
                                    <!-- TODO: Update with your arXiv paper ID -->
                                    <span class="link-block">
                                        <a href="https://arxiv.org/pdf/<ARXIV PAPER ID>.pdf" target="_blank"
                                            class="external-link button is-normal is-rounded is-dark">
                                            <span class="icon">
                                                <i class="fas fa-file-pdf"></i>
                                            </span>
                                            <span>Paper</span>
                                        </a>
                                    </span>

                                    <!-- TODO: Add your supplementary material PDF or remove this section
                                    <span class="link-block">
                                        <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                                            class="external-link button is-normal is-rounded is-dark">
                                            <span class="icon">
                                                <i class="fas fa-file-pdf"></i>
                                            </span>
                                            <span>Supplementary</span>
                                        </a>
                                    </span> -->

                                    <!-- TODO: Replace with your GitHub repository URL -->
                                    <span class="link-block">
                                        <a href="https://github.com/Jana-Z/mentis-oculis" target="_blank"
                                            class="external-link button is-normal is-rounded is-dark">
                                            <span class="icon">
                                                <i class="fab fa-github"></i>
                                            </span>
                                            <span>Code</span>
                                        </a>
                                    </span>

                                    <!-- TODO: Update with your arXiv paper ID -->
                                    <span class="link-block">
                                        <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                                            class="external-link button is-normal is-rounded is-dark">
                                            <span class="icon">
                                                <i class="ai ai-arxiv"></i>
                                            </span>
                                            <span>arXiv</span>
                                        </a>
                                    </span>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </section>


        <!-- Teaser video-->
        <section class="hero teaser">
            <div class="container is-max-desktop">
                <div class="hero-body">
                    <!-- TODO: Replace with your teaser video -->
                    <video poster="" id="tree" autoplay controls muted loop height="100%" preload="metadata">
                        <!-- TODO: Add your video file path here -->
                        <source src="static/videos/banner_video.mp4" type="video/mp4">
                    </video>
                    <!-- TODO: Replace with your video description -->
                    <h2 class="subtitle has-text-centered">
                        MentisOculi allows to compare different visual reasoning strategies across different model
                        families.
                    </h2>
                </div>
            </div>
        </section>
        <!-- End teaser video -->

        <section class="hero teaser">
            <div class="container is-max-desktop is-centered is-light">
                <div class="hero-body has-text-centered">
                    <h2 class="title is-3"><span style="font-variant: small-caps;">MentisOculi</span> comprises five
                        visual reasoning tasks designed to be best-solved with mental imagery</h2>
                    <img src="static/images/mentis-oculi-teaser.png" alt="Teaser image" loading="lazy" />
                    Collectively, the tasks require models to solve multi-step reasoning problems with geometric
                    constraints.
                    Success hinges on the ability to maintain a visual representation with high fidelity and
                    consistent geometry under affine transformations.
                    Each task is procedurally generated across five difficulty levels, scaling with the number of
                    operations required from one (left) to five (right)
                    </p>
                </div>
            </div>
        </section>

        <!-- Paper abstract -->
        <section class="section hero">
            <div class="container is-max-desktop">
                <div class="columns is-centered has-text-centered">
                    <div class="column is-four-fifths">
                        <h2 class="title is-3">Abstract</h2>
                        <div class="content has-text-justified">
                            <!-- TODO: Replace with your paper abstract -->
                            <p>
                                Frontier models are transitioning from multimodal large language models (MLLMs) that
                                merely ingest visual information to unified multimodal models (UMMs) capable of native
                                interleaved generation.
                                This shift has sparked interest in using intermediate visualizations as a reasoning aid,
                                akin to human mental imagery.
                                Central to this idea is the ability to form, maintain, and manipulate visual
                                representations in a goal-oriented manner.
                                To evaluate and probe this capability, we develop MentisOculi, a procedural, stratified
                                suite of multi-step reasoning problems amenable to visual solution, tuned to challenge
                                frontier models.
                                Evaluating visual strategies ranging from latent tokens to explicit generated imagery,
                                we find they generally fail to improve performance. Analysis of UMMs specifically
                                exposes a critical limitation: While they possess the textual reasoning capacity to
                                solve a task and can sometimes generate correct visuals, they suffer from compounding
                                generation errors and fail to leverage even ground-truth visualizations.
                                Our findings suggest that despite their inherent appeal, visual thoughts do not yet
                                benefit model reasoning. MentisOculi establishes the necessary foundation to analyze and
                                close this gap across diverse model families.
                            </p>
                        </div>
                    </div>
                </div>
            </div>
        </section>
        <!-- End paper abstract -->


        <!-- Image carousel -->
        <section class="hero is-small is-light">
            <div class="hero-body">
                <div class="container has-text-centered">
                    <h2 class="title is-3">Results</h2>
                    <div id="results-carousel" class="carousel results-carousel">

                        <div class="item">
                            <img src="static/images/all_tasks_135.jpg" alt="Performance across all tasks"
                                loading="lazy" />
                            <h2 class="subtitle has-text-centered">
                                <strong>
                                    Takeaway 1: <span style="font-variant: small-caps;">MentisOculi</span> is far from
                                    saturated
                                </strong>
                                <br>
                                <span style="color:#2EC4B6; font-weight:bold;">MLLMs</span> and
                                <span style="color:#E85D75; font-weight:bold;">UMMs</span>
                                display similar failure patterns. Performance degrades consistently with difficulty and
                                falls below chance at Level&nbsp;5. This highlights the fundamental
                                limitations of
                                current state-of-the-art models in solving multi-step visual reasoning tasks.
                            </h2>
                        </div>

                        <div class="item">
                            <img src="static/images/model_families.jpg" alt="Comparison of model families"
                                loading="lazy" />
                            <h2 class="subtitle has-text-centered">
                                <strong>
                                    Takeaway 2: Explicit visual thought is currently ineffective
                                </strong>
                                <br>
                                We find no evidence that self-generated imagery improves text-only reasoning.
                                <span style="color:#C46A2E; font-weight:bold;">Latent visual reasoning</span> (Mirage)
                                offers only brittle gains, while
                                <span style="color:#E85D75; font-weight:bold;">UMMs</span> often underperform their
                                text-only counterparts.
                                <span style="color:#C42E88; font-weight:bold;">Video models</span> (Veo-3) fail rapidly
                                as complexity increases.
                            </h2>
                        </div>

                        <div class="item">
                            <img src="static/images/text_vs_image_input.jpg" alt="Text transcription vs Image input"
                                loading="lazy" />
                            <h2 class="subtitle has-text-centered">
                                <strong>
                                    Takeaway 3: Models possess the <em>competence</em> to solve the tasks
                                </strong>
                                <br>
                                When prompted with a precise text transcription rather than an image,
                                <span style="color:#2EC4B6; font-weight:bold;">MLLMs</span> like Gemini&nbsp;3 and GPT-5
                                can solve RushHour on par with humans. This proves that the failure
                                stems from visual processing and planning, not a lack of logical reasoning
                                capacity.
                            </h2>
                        </div>

                        <div class="item">
                            <img src="static/images/interleaved-all-tasks-135.jpg"
                                alt="UMM Generation and Interpretation errors" loading="lazy" />
                            <h2 class="subtitle has-text-centered">
                                <strong>
                                    Why do <span style="color:#E85D75; font-weight:bold;">UMMs</span> fail? A dual
                                    issue.
                                </strong>
                                <br>
                                Visual reasoning suffers from <em>generation errors</em> (bad images) and
                                <em>interpretation errors</em>.
                                Even when provided with correct "oracle" visuals, models often fail to use them as
                                actionable evidence.
                                This suggests architectures cannot yet effectively bridge the gap between generation and
                                reasoning.
                            </h2>
                        </div>

                    </div>
                </div>
            </div>
        </section>



        <!--BibTex citation -->
        <section class="section" id="BibTeX">
            <div class="container is-max-desktop content">
                <div class="bibtex-header">
                    <h2 class="title">BibTeX</h2>
                    <button class="copy-bibtex-btn" onclick="copyBibTeX()" title="Copy BibTeX to clipboard">
                        <i class="fas fa-copy"></i>
                        <span class="copy-text">Copy</span>
                    </button>
                </div>
                <pre id="bibtex-code"><code>@article{zeller2026mentisoculi,
                    title={{MENTISOCULI}: Revealing the Limits of Reasoning with Mental Imagery},
                    author={Zeller, Jana and Wiedemer, Thadd{\"a}us and Li, Fanfei and Klein, Thomas and Mayilvahanan, Prasanna and Bethge, Matthias and Wichmann, Felix and Cotterell, Ryan and Brendel, Wieland},
                    journal={arXiv preprint},
                    year={2026},
                    note={Preprint. January 31, 2026},
                    url={https://jana-z.github.io/mentis-oculis}
                  }
                </code></pre>
            </div>
        </section>
        <!--End BibTex citation -->


        <footer class="footer">
            <div class="container">
                <div class="columns is-centered">
                    <div class="column is-8">
                        <div class="content">

                            <p>
                                This page was built using the <a
                                    href="https://github.com/eliahuhorwitz/Academic-project-page-template"
                                    target="_blank">Academic Project Page Template</a> which was adopted from the <a
                                    href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
                                You are free to borrow the source code of this website, we just ask that you link back
                                to this page in the footer. <br> This website is licensed under a <a rel="license"
                                    href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
                                    Commons Attribution-ShareAlike 4.0 International License</a>.
                            </p>

                        </div>
                    </div>
                </div>
            </div>
        </footer>

        <!-- Statcounter tracking code -->

        <!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

        <!-- End of Statcounter Code -->

</body>

</html>